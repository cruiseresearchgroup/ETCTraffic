{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d077afd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:06:18.735473Z",
     "start_time": "2024-05-06T12:06:18.326908Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e9edc97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:06:19.185155Z",
     "start_time": "2024-05-06T12:06:18.736470Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Station</th>\n",
       "      <th>District</th>\n",
       "      <th>Freeway #</th>\n",
       "      <th>Direction of Travel</th>\n",
       "      <th>Lane Type</th>\n",
       "      <th>Station Length</th>\n",
       "      <th>Samples</th>\n",
       "      <th>% Observed</th>\n",
       "      <th>Total Flow</th>\n",
       "      <th>Avg Occupancy</th>\n",
       "      <th>Avg Speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02/28/2018 00:00:00</td>\n",
       "      <td>1100313</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/28/2018 00:00:00</td>\n",
       "      <td>1100323</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/28/2018 00:00:00</td>\n",
       "      <td>1100326</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02/28/2018 00:00:00</td>\n",
       "      <td>1100330</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/28/2018 00:00:00</td>\n",
       "      <td>1100333</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433435</th>\n",
       "      <td>02/28/2018 23:55:00</td>\n",
       "      <td>1126770</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>W</td>\n",
       "      <td>ML</td>\n",
       "      <td>0.424</td>\n",
       "      <td>24</td>\n",
       "      <td>100</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>67.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433436</th>\n",
       "      <td>02/28/2018 23:55:00</td>\n",
       "      <td>1126790</td>\n",
       "      <td>11</td>\n",
       "      <td>805</td>\n",
       "      <td>S</td>\n",
       "      <td>ML</td>\n",
       "      <td>0.805</td>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>64.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433437</th>\n",
       "      <td>02/28/2018 23:55:00</td>\n",
       "      <td>1126791</td>\n",
       "      <td>11</td>\n",
       "      <td>805</td>\n",
       "      <td>S</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433438</th>\n",
       "      <td>02/28/2018 23:55:00</td>\n",
       "      <td>1126792</td>\n",
       "      <td>11</td>\n",
       "      <td>805</td>\n",
       "      <td>S</td>\n",
       "      <td>HV</td>\n",
       "      <td>2.935</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>65.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433439</th>\n",
       "      <td>02/28/2018 23:55:00</td>\n",
       "      <td>1126796</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>N</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433440 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp  Station  District  Freeway # Direction of Travel  \\\n",
       "0       02/28/2018 00:00:00  1100313        11          5                   N   \n",
       "1       02/28/2018 00:00:00  1100323        11          5                   N   \n",
       "2       02/28/2018 00:00:00  1100326        11          5                   N   \n",
       "3       02/28/2018 00:00:00  1100330        11          5                   N   \n",
       "4       02/28/2018 00:00:00  1100333        11          5                   N   \n",
       "...                     ...      ...       ...        ...                 ...   \n",
       "433435  02/28/2018 23:55:00  1126770        11         52                   W   \n",
       "433436  02/28/2018 23:55:00  1126790        11        805                   S   \n",
       "433437  02/28/2018 23:55:00  1126791        11        805                   S   \n",
       "433438  02/28/2018 23:55:00  1126792        11        805                   S   \n",
       "433439  02/28/2018 23:55:00  1126796        11          5                   N   \n",
       "\n",
       "       Lane Type  Station Length  Samples  % Observed  Total Flow  \\\n",
       "0             FR             NaN       10         100         6.0   \n",
       "1             FR             NaN       20           0         NaN   \n",
       "2             FR             NaN       20         100        10.0   \n",
       "3             FR             NaN       20         100         2.0   \n",
       "4             FR             NaN       10         100         6.0   \n",
       "...          ...             ...      ...         ...         ...   \n",
       "433435        ML           0.424       24         100        47.0   \n",
       "433436        ML           0.805       36         100       109.0   \n",
       "433437        FR             NaN        9         100         8.0   \n",
       "433438        HV           2.935        9         100         0.0   \n",
       "433439        FR             NaN        0           0         NaN   \n",
       "\n",
       "        Avg Occupancy  Avg Speed  \n",
       "0                 NaN        NaN  \n",
       "1                 NaN        NaN  \n",
       "2                 NaN        NaN  \n",
       "3                 NaN        NaN  \n",
       "4                 NaN        NaN  \n",
       "...               ...        ...  \n",
       "433435         0.0132       67.1  \n",
       "433436         0.0268       64.1  \n",
       "433437            NaN        NaN  \n",
       "433438         0.0000       65.1  \n",
       "433439            NaN        NaN  \n",
       "\n",
       "[433440 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2001 = pd.read_csv('./pems11/d11_text_station_5min_2018_02_28.txt.gz',header=None, usecols=range(12))\n",
    "# Assign column names based on the provided headers\n",
    "column_names = [\n",
    "    \"Timestamp\", \"Station\", \"District\", \"Freeway #\", \n",
    "    \"Direction of Travel\", \"Lane Type\", \"Station Length\", \n",
    "    \"Samples\", \"% Observed\", \"Total Flow\", \"Avg Occupancy\", \"Avg Speed\"\n",
    "]\n",
    "\n",
    "# Assign column names to the dataframe\n",
    "data2001.columns = column_names\n",
    "data2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8db779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:06:22.055736Z",
     "start_time": "2024-05-06T12:06:22.051505Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1100313, 1100326, 1100330, 1100340, 1100348, 1100353, 1100363,\n",
       "       1100369, 1100372, 1100377, 1100381, 1100384, 1100387, 1100390,\n",
       "       1100393, 1100396, 1100407, 1100416, 1100434, 1100451, 1100454,\n",
       "       1100461, 1100475, 1100478, 1100486, 1100491, 1100504, 1100524,\n",
       "       1100530, 1100533, 1100534, 1100538, 1100542, 1100549, 1100553,\n",
       "       1100560, 1100568, 1100571, 1100575, 1100578, 1100592, 1100595,\n",
       "       1100598, 1100601, 1100604, 1100607, 1100697, 1100703, 1100706,\n",
       "       1100711, 1100714, 1100729, 1100732, 1100737, 1100741, 1100745,\n",
       "       1100750, 1100757, 1100776, 1100781, 1100787, 1100796, 1100802,\n",
       "       1100805, 1100810, 1100813, 1100850, 1100860, 1100863, 1100865,\n",
       "       1100868, 1107608, 1107611, 1108285, 1108286, 1108287, 1108288,\n",
       "       1108289, 1108290, 1108291, 1108292, 1108293, 1108294, 1108295,\n",
       "       1108296, 1108297, 1108299, 1108300, 1108301, 1108302, 1108303,\n",
       "       1108304, 1108305, 1108306, 1108307, 1108308, 1108309, 1108310,\n",
       "       1108311, 1108312, 1108313, 1108314, 1108315, 1108316, 1108317,\n",
       "       1108318, 1108321, 1108323, 1108324, 1108325, 1108326, 1108327,\n",
       "       1108328, 1108329, 1108330, 1108331, 1108332, 1108333, 1108334,\n",
       "       1108341, 1108342, 1108343, 1108344, 1108345, 1108346, 1108347,\n",
       "       1108348, 1108350, 1108351, 1108352, 1108353, 1108354, 1108355,\n",
       "       1108356, 1108357, 1108358, 1108359, 1108366, 1108367, 1108369,\n",
       "       1108371, 1108372, 1108373, 1108374, 1108375, 1108376, 1108377,\n",
       "       1108378, 1108379, 1108380, 1108381, 1108385, 1108386, 1108387,\n",
       "       1108388, 1108389, 1108390, 1108391, 1108392, 1108393, 1108394,\n",
       "       1108398, 1108399, 1108400, 1108401, 1108402, 1108410, 1108411,\n",
       "       1108413, 1108414, 1108415, 1108416, 1108417, 1108418, 1108419,\n",
       "       1108420, 1108421, 1108422, 1108423, 1108424, 1108425, 1108426,\n",
       "       1108429, 1108430, 1108432, 1108433, 1108434, 1108435, 1108436,\n",
       "       1108437, 1108438, 1108439, 1108440, 1108441, 1108442, 1108444,\n",
       "       1108445, 1108447, 1108448, 1108449, 1108450, 1108451, 1108452,\n",
       "       1108453, 1108455, 1108459, 1108460, 1108461, 1108462, 1108463,\n",
       "       1108464, 1108465, 1108466, 1108467, 1108468, 1108469, 1108471,\n",
       "       1108472, 1108473, 1108474, 1108475, 1108476, 1108477, 1108478,\n",
       "       1108479, 1108480, 1108483, 1108484, 1108485, 1108486, 1108487,\n",
       "       1108488, 1108489, 1108490, 1108491, 1108492, 1108493, 1108494,\n",
       "       1108495, 1108496, 1108497, 1108498, 1108499, 1108500, 1108501,\n",
       "       1108502, 1108503, 1108504, 1108505, 1108506, 1108507, 1108508,\n",
       "       1108509, 1108510, 1108511, 1108512, 1108513, 1108516, 1108517,\n",
       "       1108518, 1108519, 1108520, 1108521, 1108522, 1108523, 1108524,\n",
       "       1108525, 1108528, 1108529, 1108530, 1108531, 1108532, 1108534,\n",
       "       1108535, 1108536, 1108537, 1108538, 1108539, 1108540, 1108541,\n",
       "       1108542, 1108543, 1108544, 1108545, 1108546, 1108547, 1108548,\n",
       "       1108549, 1108550, 1108553, 1108554, 1108555, 1108556, 1108557,\n",
       "       1108558, 1108559, 1108560, 1108561, 1108562, 1108563, 1108564,\n",
       "       1108565, 1108567, 1108568, 1108569, 1108570, 1108571, 1108572,\n",
       "       1108573, 1108574, 1108577, 1108578, 1108579, 1108580, 1108581,\n",
       "       1108585, 1108586, 1108587, 1108589, 1108590, 1108591, 1108592,\n",
       "       1108593, 1108594, 1108595, 1108596, 1108597, 1108598, 1108599,\n",
       "       1108600, 1108601, 1108602, 1108603, 1108604, 1108605, 1108606,\n",
       "       1108607, 1108608, 1108609, 1108610, 1108611, 1108612, 1108613,\n",
       "       1108614, 1108615, 1108616, 1108617, 1108618, 1108619, 1108620,\n",
       "       1108621, 1108622, 1108623, 1108624, 1108625, 1108626, 1108627,\n",
       "       1108628, 1108629, 1108630, 1108631, 1108632, 1108633, 1108634,\n",
       "       1108635, 1108636, 1108637, 1108638, 1108639, 1108640, 1108642,\n",
       "       1108643, 1108644, 1108645, 1108646, 1108649, 1108650, 1108651,\n",
       "       1108652, 1108653, 1108654, 1108655, 1108656, 1108657, 1108658,\n",
       "       1108659, 1108660, 1108661, 1108662, 1108663, 1108664, 1108665,\n",
       "       1108666, 1108667, 1108668, 1108669, 1108670, 1108671, 1108672,\n",
       "       1108673, 1108674, 1108675, 1108676, 1108677, 1108680, 1108681,\n",
       "       1108682, 1108683, 1108684, 1108685, 1108686, 1108687, 1108688,\n",
       "       1108689, 1108690, 1108691, 1108692, 1108693, 1108694, 1108697,\n",
       "       1108698, 1108700, 1108701, 1108702, 1108703, 1108704, 1108705,\n",
       "       1108706, 1108707, 1108708, 1108709, 1108710, 1108711, 1108712,\n",
       "       1108713, 1108714, 1108715, 1108716, 1108717, 1108718, 1108719,\n",
       "       1108720, 1108721, 1108722, 1108723, 1108724, 1108725, 1108726,\n",
       "       1108727, 1108728, 1108729, 1108731, 1108732, 1108733, 1108734,\n",
       "       1108735, 1108736, 1108737, 1108738, 1108747, 1108748, 1108749,\n",
       "       1108750, 1108751, 1108752, 1108753, 1108754, 1108755, 1108756,\n",
       "       1108757, 1108758, 1108759, 1108760, 1108761, 1108762, 1108763,\n",
       "       1108764, 1108765, 1108766, 1108767, 1108768, 1108771, 1108772,\n",
       "       1108773, 1108774, 1111509, 1111514, 1111525, 1111527, 1111530,\n",
       "       1111531, 1111532, 1111534, 1111535, 1111539, 1111540, 1111541,\n",
       "       1111542, 1111543, 1111544, 1111545, 1111546, 1111547, 1111548,\n",
       "       1111549, 1111550, 1111552, 1111553, 1111554, 1111555, 1111556,\n",
       "       1111557, 1111558, 1111559, 1111564, 1111565, 1111566, 1111567,\n",
       "       1111568, 1111570, 1111572], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm = np.load('pems11_comm.npy')\n",
    "comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8681a402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:13:42.180768Z",
     "start_time": "2024-05-06T12:07:11.549420Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import calendar\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def read_and_process_data(file_path):\n",
    "    try:\n",
    "        # Read the CSV file, specifying the usecols parameter to only load the columns of interest\n",
    "        data = pd.read_csv(\n",
    "            file_path, \n",
    "            header=None, \n",
    "            usecols=[0, 1, 9],  # Column indexes for Timestamp, Station, and Total Flow\n",
    "            names=['Timestamp', 'Station', 'Avg Flow'],  # Assigning column names\n",
    "            compression='gzip'\n",
    "        )\n",
    "\n",
    "        # Convert the Timestamp column to datetime format and set it as the index\n",
    "        data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "        data.set_index('Timestamp', inplace=True)\n",
    "\n",
    "        # Pivot the table to get Stations as columns and Total Flow as cell values\n",
    "        pivot_data = data.pivot(columns='Station', values='Avg Flow')\n",
    "\n",
    "        return pivot_data\n",
    "\n",
    "    except EOFError:\n",
    "        print(f\"Error processing file: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# Read the data for February 28 to get the column names (Station IDs)\n",
    "# feb_28_data = read_and_process_data('./pems03/d03_text_station_5min_2018_02_28.txt.gz')\n",
    "# feb_28_columns = feb_28_data.columns\n",
    "\n",
    "# Initialize a list to hold the data for all days\n",
    "all_data = []\n",
    "\n",
    "start_date = date(2002, 9, 1)  # 开始日期\n",
    "# end_date = date(2012, 10, 2)   # 结束日期\n",
    "end_date = date(2024, 3, 20)   # 结束日期\n",
    "current_date = start_date\n",
    "all_data = []                  # 存储所有数据的列表\n",
    "\n",
    "while current_date <= end_date:\n",
    "    year = current_date.year\n",
    "    month = current_date.month\n",
    "    day = current_date.day\n",
    "    file_path = f'./pems11/d11_text_station_5min_{year}_{month:02d}_{day:02d}.txt.gz'\n",
    "#     if year==2012 and month==10 and day==1:\n",
    "#         continue\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}, skipping...\")\n",
    "        current_date += timedelta(days=1)\n",
    "        continue\n",
    "    \n",
    "    day_data = read_and_process_data(file_path)\n",
    "    \n",
    "    # 使用字典存储新列，以便一次性添加到数据帧\n",
    "    new_columns = {}\n",
    "    for col in comm:\n",
    "        if col not in day_data:\n",
    "            new_columns[col] = 0  # 假设使用0填充缺失的列\n",
    "\n",
    "    # 使用 pd.concat 添加所有新列\n",
    "    if new_columns:\n",
    "        new_data = pd.DataFrame(new_columns, index=day_data.index)\n",
    "        day_data = pd.concat([day_data, new_data], axis=1)\n",
    "\n",
    "    # 将列重新排序以匹配2月28日的顺序\n",
    "    day_data = day_data[comm]\n",
    "    \n",
    "    all_data.append(day_data)\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Concatenate all daily data into a single DataFrame\n",
    "combined_data = pd.concat(all_data)\n",
    "\n",
    "# Fill any remaining missing values with 0\n",
    "combined_data.fillna(0, inplace=True)\n",
    "\n",
    "# The combined_data DataFrame now holds the merged data for January 1 to February 28\n",
    "# with columns aligned to February 28 and missing values filled with 0\n",
    "# Note: The code execution is commented out to prevent execution in this environment.\n",
    "# combined_data.head()\n",
    "combined_data.to_csv('pems11_all_common_flow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c66bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T00:14:12.508486Z",
     "start_time": "2024-04-16T00:14:10.007650Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fb91996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:52:41.796325Z",
     "start_time": "2024-05-23T16:48:29.319756Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_47676\\3132681090.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  pems11_hourly = pems11.resample('H').sum()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pems11 = pd.read_csv('pems11_all_common_flow.csv')\n",
    "\n",
    "# Convert the 'date' column to datetime\n",
    "pems11['date'] = pd.to_datetime(pems11['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "pems11.set_index('date', inplace=True)\n",
    "\n",
    "# Resample to hourly data and aggregate using sum\n",
    "pems11_hourly = pems11.resample('H').sum()\n",
    "\n",
    "# Reset the index if you want the 'date' column back\n",
    "pems11_hourly.reset_index(inplace=True)\n",
    "pems11_hourly.to_csv('pems11_h.csv',index=False)\n",
    "\n",
    "\n",
    "# Convert the 'date' column to datetime\n",
    "pems11 = pd.read_csv('pems11_all_common_flow.csv')\n",
    "pems11['date'] = pd.to_datetime(pems11['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "pems11.set_index('date', inplace=True)\n",
    "\n",
    "# Resample to hourly data and aggregate using sum\n",
    "pems11_daily = pems11.resample('D').sum()\n",
    "\n",
    "# Reset the index if you want the 'date' column back\n",
    "pems11_daily.reset_index(inplace=True)\n",
    "pems11_daily.to_csv('pems11_d.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab21243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch110",
   "language": "python",
   "name": "torch110"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
