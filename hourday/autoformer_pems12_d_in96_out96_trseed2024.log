Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5489
val 717
test 1528
	iters: 100, epoch: 1 | loss: 0.4926603
	speed: 0.1407s/iter; left time: 4797.2391s
Epoch: 1 cost time: 23.76870059967041
Epoch: 1, Steps: 171 | Train Loss: 0.6139164 Vali Loss: 0.6380370 Test Loss: 1.5674464
Validation loss decreased (inf --> 0.638037).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.5849540
	speed: 0.4611s/iter; left time: 15644.4155s
Epoch: 2 cost time: 24.381494998931885
Epoch: 2, Steps: 171 | Train Loss: 0.5461155 Vali Loss: 0.6366274 Test Loss: 1.5697582
Validation loss decreased (0.638037 --> 0.636627).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.4255835
	speed: 0.4572s/iter; left time: 15433.7480s
Epoch: 3 cost time: 23.248056411743164
Epoch: 3, Steps: 171 | Train Loss: 0.4500568 Vali Loss: 0.6563277 Test Loss: 1.6154475
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 4 | loss: 0.3475430
	speed: 0.4942s/iter; left time: 16597.6490s
Epoch: 4 cost time: 23.21323561668396
Epoch: 4, Steps: 171 | Train Loss: 0.3683038 Vali Loss: 0.6597577 Test Loss: 1.6389045
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 5 | loss: 0.3043874
	speed: 0.4527s/iter; left time: 15128.7074s
Epoch: 5 cost time: 23.371081590652466
Epoch: 5, Steps: 171 | Train Loss: 0.2990533 Vali Loss: 0.6773539 Test Loss: 1.6591234
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 6 | loss: 0.2427890
	speed: 0.4536s/iter; left time: 15080.9464s
Epoch: 6 cost time: 23.178664445877075
Epoch: 6, Steps: 171 | Train Loss: 0.2534490 Vali Loss: 0.6899529 Test Loss: 1.6633351
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 7 | loss: 0.2173177
	speed: 0.4507s/iter; left time: 14908.4658s
Epoch: 7 cost time: 23.13544726371765
Epoch: 7, Steps: 171 | Train Loss: 0.2246771 Vali Loss: 0.7002971 Test Loss: 1.6701392
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1528
test shape: (1528, 1, 96, 1543) (1528, 1, 96, 1543)
test shape: (1528, 96, 1543) (1528, 96, 1543)
mse:1.5697531700134277, mae:0.6084513664245605
>>>>>>>Overall time: 373 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5489
val 717
test 1528
Traceback (most recent call last):
  File "run.py", line 158, in <module>
    exp.train(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 146, in train
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 147, in forward
    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 102, in forecast
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 161, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 41, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 255, in forward
    return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
RuntimeError: Given groups=1, weight of size [128, 1543, 3], expected input[32, 867, 98] to have 1543 channels, but got 867 channels instead
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=867, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=867, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=867, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5489
val 717
test 1528
	iters: 100, epoch: 1 | loss: 0.6148282
	speed: 0.1385s/iter; left time: 4724.1495s
Epoch: 1 cost time: 22.923707008361816
Epoch: 1, Steps: 171 | Train Loss: 0.5889666 Vali Loss: 0.5838986 Test Loss: 1.3068764
Validation loss decreased (inf --> 0.583899).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.4730079
	speed: 0.4328s/iter; left time: 14686.3332s
Epoch: 2 cost time: 22.147783517837524
Epoch: 2, Steps: 171 | Train Loss: 0.4836596 Vali Loss: 0.5738786 Test Loss: 1.3476160
Validation loss decreased (0.583899 --> 0.573879).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.3821190
	speed: 0.4263s/iter; left time: 14389.8425s
Epoch: 3 cost time: 21.555495738983154
Epoch: 3, Steps: 171 | Train Loss: 0.4014766 Vali Loss: 0.5814670 Test Loss: 1.3466305
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 4 | loss: 0.3125239
	speed: 0.4261s/iter; left time: 14312.0167s
Epoch: 4 cost time: 21.69949197769165
Epoch: 4, Steps: 171 | Train Loss: 0.3245984 Vali Loss: 0.6053460 Test Loss: 1.3729893
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 5 | loss: 0.2680722
	speed: 0.4260s/iter; left time: 14234.3820s
Epoch: 5 cost time: 21.70799970626831
Epoch: 5, Steps: 171 | Train Loss: 0.2707771 Vali Loss: 0.6219569 Test Loss: 1.3996934
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 6 | loss: 0.2291217
	speed: 0.4241s/iter; left time: 14100.2303s
Epoch: 6 cost time: 21.55944299697876
Epoch: 6, Steps: 171 | Train Loss: 0.2299376 Vali Loss: 0.6174994 Test Loss: 1.3863708
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 7 | loss: 0.1859118
	speed: 0.4273s/iter; left time: 14132.3688s
Epoch: 7 cost time: 21.795618295669556
Epoch: 7, Steps: 171 | Train Loss: 0.2004510 Vali Loss: 0.6336547 Test Loss: 1.3929852
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1528
test shape: (1528, 1, 96, 867) (1528, 1, 96, 867)
test shape: (1528, 96, 867) (1528, 96, 867)
mse:1.3476142883300781, mae:0.6493383049964905
>>>>>>>Overall time: 327 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
