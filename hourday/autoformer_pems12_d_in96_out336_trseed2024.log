Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_336_Autoformer_custom_ftM_sl96_ll48_pl336_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5249
val 477
test 1288
	iters: 100, epoch: 1 | loss: 0.8267511
	speed: 0.2682s/iter; left time: 8770.4450s
Epoch: 1 cost time: 43.74910235404968
Epoch: 1, Steps: 164 | Train Loss: 0.8544813 Vali Loss: 0.9035287 Test Loss: 2.0446680
Validation loss decreased (inf --> 0.903529).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.8415703
	speed: 0.7501s/iter; left time: 24406.0962s
Epoch: 2 cost time: 43.594242811203
Epoch: 2, Steps: 164 | Train Loss: 0.7915975 Vali Loss: 0.8960743 Test Loss: 2.0447657
Validation loss decreased (0.903529 --> 0.896074).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.7691193
	speed: 0.7081s/iter; left time: 22923.9423s
Epoch: 3 cost time: 43.93150854110718
Epoch: 3, Steps: 164 | Train Loss: 0.7466371 Vali Loss: 0.9186769 Test Loss: 2.0875499
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 4 | loss: 0.6516656
	speed: 0.7152s/iter; left time: 23035.7889s
Epoch: 4 cost time: 44.3463499546051
Epoch: 4, Steps: 164 | Train Loss: 0.6952338 Vali Loss: 0.9268913 Test Loss: 2.0607584
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 5 | loss: 0.6708144
	speed: 0.7272s/iter; left time: 23304.4273s
Epoch: 5 cost time: 43.50099229812622
Epoch: 5, Steps: 164 | Train Loss: 0.7126246 Vali Loss: 0.9092547 Test Loss: 2.0888214
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 6 | loss: 0.5878565
	speed: 0.7097s/iter; left time: 22625.6524s
Epoch: 6 cost time: 43.87055850028992
Epoch: 6, Steps: 164 | Train Loss: 0.6311210 Vali Loss: 0.9589486 Test Loss: 2.2120810
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 7 | loss: 0.5074228
	speed: 0.7067s/iter; left time: 22415.7966s
Epoch: 7 cost time: 43.97310185432434
Epoch: 7, Steps: 164 | Train Loss: 0.5609393 Vali Loss: 0.9685866 Test Loss: 2.1171288
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_336_Autoformer_custom_ftM_sl96_ll48_pl336_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1288
test shape: (1288, 1, 336, 1543) (1288, 1, 336, 1543)
test shape: (1288, 336, 1543) (1288, 336, 1543)
mse:2.0447633266448975, mae:0.6816015243530273
>>>>>>>Overall time: 556 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_336_Autoformer_custom_ftM_sl96_ll48_pl336_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5249
val 477
test 1288
Traceback (most recent call last):
  File "run.py", line 158, in <module>
    exp.train(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 146, in train
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 147, in forward
    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 102, in forecast
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 161, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 41, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 255, in forward
    return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
RuntimeError: Given groups=1, weight of size [128, 1543, 3], expected input[32, 867, 98] to have 1543 channels, but got 867 channels instead
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=867, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=867, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=867, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_336_Autoformer_custom_ftM_sl96_ll48_pl336_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5249
val 477
test 1288
	iters: 100, epoch: 1 | loss: 0.6794329
	speed: 0.2224s/iter; left time: 7273.1295s
Epoch: 1 cost time: 36.08958578109741
Epoch: 1, Steps: 164 | Train Loss: 0.8119809 Vali Loss: 0.8995327 Test Loss: 1.8031882
Validation loss decreased (inf --> 0.899533).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.8489448
	speed: 0.5858s/iter; left time: 19061.0545s
Epoch: 2 cost time: 35.953707218170166
Epoch: 2, Steps: 164 | Train Loss: 0.7397725 Vali Loss: 0.8943471 Test Loss: 1.8008503
Validation loss decreased (0.899533 --> 0.894347).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.6466095
	speed: 0.5796s/iter; left time: 18764.3134s
Epoch: 3 cost time: 35.591575145721436
Epoch: 3, Steps: 164 | Train Loss: 0.6844521 Vali Loss: 0.8955274 Test Loss: 1.8112115
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 4 | loss: 0.6456224
	speed: 0.5834s/iter; left time: 18789.8063s
Epoch: 4 cost time: 35.879865407943726
Epoch: 4, Steps: 164 | Train Loss: 0.6215096 Vali Loss: 0.9110657 Test Loss: 1.8728660
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 5 | loss: 0.5067949
	speed: 0.5788s/iter; left time: 18547.2090s
Epoch: 5 cost time: 35.27461552619934
Epoch: 5, Steps: 164 | Train Loss: 0.5288845 Vali Loss: 0.9186992 Test Loss: 1.9003546
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 6 | loss: 0.6756366
	speed: 0.5849s/iter; left time: 18648.7686s
Epoch: 6 cost time: 36.334392786026
Epoch: 6, Steps: 164 | Train Loss: 0.6245033 Vali Loss: 0.8960893 Test Loss: 1.8301032
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 7 | loss: 0.5984719
	speed: 0.5838s/iter; left time: 18515.1123s
Epoch: 7 cost time: 35.761531591415405
Epoch: 7, Steps: 164 | Train Loss: 0.6504676 Vali Loss: 0.9128705 Test Loss: 1.8217137
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_336_Autoformer_custom_ftM_sl96_ll48_pl336_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1288
test shape: (1288, 1, 336, 867) (1288, 1, 336, 867)
test shape: (1288, 336, 867) (1288, 336, 867)
mse:1.8008410930633545, mae:0.7459033131599426
>>>>>>>Overall time: 438 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
