Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_720', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=720, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_720_Autoformer_custom_ftM_sl96_ll48_pl720_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4865
val 93
test 904
	iters: 100, epoch: 1 | loss: 0.9377204
	speed: 0.4718s/iter; left time: 14297.3159s
Epoch: 1 cost time: 71.6491105556488
Epoch: 1, Steps: 152 | Train Loss: 1.0393636 Vali Loss: 0.9690377 Test Loss: 2.6246219
Validation loss decreased (inf --> 0.969038).  Saving model ...
	iters: 100, epoch: 2 | loss: 1.0223416
	speed: 0.9318s/iter; left time: 28094.1541s
Epoch: 2 cost time: 71.58334064483643
Epoch: 2, Steps: 152 | Train Loss: 0.9877053 Vali Loss: 0.9727819 Test Loss: 2.6191676
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 3 | loss: 0.9032336
	speed: 0.9360s/iter; left time: 28076.5089s
Epoch: 3 cost time: 71.82056498527527
Epoch: 3, Steps: 152 | Train Loss: 0.9696507 Vali Loss: 0.9641273 Test Loss: 2.6336584
Validation loss decreased (0.969038 --> 0.964127).  Saving model ...
	iters: 100, epoch: 4 | loss: 1.0013729
	speed: 0.9634s/iter; left time: 28751.1917s
Epoch: 4 cost time: 71.31546592712402
Epoch: 4, Steps: 152 | Train Loss: 0.9425954 Vali Loss: 0.9736741 Test Loss: 2.6304359
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 5 | loss: 0.9101955
	speed: 0.9323s/iter; left time: 27681.5750s
Epoch: 5 cost time: 71.88144659996033
Epoch: 5, Steps: 152 | Train Loss: 0.9150129 Vali Loss: 0.9684888 Test Loss: 2.6304116
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 6 | loss: 0.8075247
	speed: 0.9392s/iter; left time: 27746.0917s
Epoch: 6 cost time: 72.39592170715332
Epoch: 6, Steps: 152 | Train Loss: 0.8699731 Vali Loss: 1.0007354 Test Loss: 2.6624815
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 7 | loss: 0.8680332
	speed: 0.9298s/iter; left time: 27325.7106s
Epoch: 7 cost time: 71.21276116371155
Epoch: 7, Steps: 152 | Train Loss: 0.8248622 Vali Loss: 1.0073341 Test Loss: 2.6820540
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 8 | loss: 0.8219907
	speed: 0.9368s/iter; left time: 27390.5933s
Epoch: 8 cost time: 72.12798190116882
Epoch: 8, Steps: 152 | Train Loss: 0.7886265 Vali Loss: 1.0143721 Test Loss: 2.7200975
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_720_Autoformer_custom_ftM_sl96_ll48_pl720_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 904
test shape: (904, 1, 720, 1543) (904, 1, 720, 1543)
test shape: (904, 720, 1543) (904, 720, 1543)
mse:2.6336708068847656, mae:0.7997077703475952
>>>>>>>Overall time: 806 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_720', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=720, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_720_Autoformer_custom_ftM_sl96_ll48_pl720_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4865
val 93
test 904
Traceback (most recent call last):
  File "run.py", line 158, in <module>
    exp.train(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 146, in train
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 147, in forward
    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 102, in forecast
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 161, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 41, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 255, in forward
    return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
RuntimeError: Given groups=1, weight of size [128, 1543, 3], expected input[32, 867, 98] to have 1543 channels, but got 867 channels instead
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=867, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=867, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=867, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_720', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=720, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_720_Autoformer_custom_ftM_sl96_ll48_pl720_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4865
val 93
test 904
	iters: 100, epoch: 1 | loss: 0.9822311
	speed: 0.4040s/iter; left time: 12241.4061s
Epoch: 1 cost time: 61.157498359680176
Epoch: 1, Steps: 152 | Train Loss: 0.9772153 Vali Loss: 0.9250894 Test Loss: 2.3857489
Validation loss decreased (inf --> 0.925089).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.9710528
	speed: 0.7807s/iter; left time: 23536.4109s
Epoch: 2 cost time: 60.34719896316528
Epoch: 2, Steps: 152 | Train Loss: 0.9193795 Vali Loss: 0.9321319 Test Loss: 2.3808577
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 3 | loss: 0.8887406
	speed: 0.7837s/iter; left time: 23509.0580s
Epoch: 3 cost time: 60.703702449798584
Epoch: 3, Steps: 152 | Train Loss: 0.8728070 Vali Loss: 0.9287686 Test Loss: 2.3967981
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 4 | loss: 0.9288380
	speed: 0.7841s/iter; left time: 23401.8962s
Epoch: 4 cost time: 60.8008177280426
Epoch: 4, Steps: 152 | Train Loss: 0.8413164 Vali Loss: 0.9755126 Test Loss: 2.3770077
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 5 | loss: 0.8228499
	speed: 0.7802s/iter; left time: 23165.4561s
Epoch: 5 cost time: 60.66253042221069
Epoch: 5, Steps: 152 | Train Loss: 0.8025024 Vali Loss: 0.9639552 Test Loss: 2.3818295
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 6 | loss: 0.8102050
	speed: 0.7840s/iter; left time: 23160.6212s
Epoch: 6 cost time: 60.51621675491333
Epoch: 6, Steps: 152 | Train Loss: 0.7525144 Vali Loss: 1.0578754 Test Loss: 2.5310104
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_720_Autoformer_custom_ftM_sl96_ll48_pl720_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 904
test shape: (904, 1, 720, 867) (904, 1, 720, 867)
test shape: (904, 720, 867) (904, 720, 867)
mse:2.3857507705688477, mae:0.8677005171775818
>>>>>>>Overall time: 500 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
