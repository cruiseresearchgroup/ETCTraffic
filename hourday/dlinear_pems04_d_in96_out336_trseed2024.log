Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='pems04_d.csv', dec_in=822, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='DLinear', model_id='pems04_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Traceback (most recent call last):
  File "run.py", line 158, in <module>
    exp.train(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 84, in train
    train_data, train_loader = self._get_data(flag='train')
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 31, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/g/data/hn98/du/exlts/hourdayweek/data_provider/data_factory.py", line 74, in data_provider
    data_set = Data(
  File "/g/data/hn98/du/exlts/hourdayweek/data_provider/data_loader.py", line 320, in __init__
    self.__read_data__()
  File "/g/data/hn98/du/exlts/hourdayweek/data_provider/data_loader.py", line 324, in __read_data__
    df_raw = pd.read_csv(os.path.join(self.root_path,
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/util/_decorators.py", line 211, in wrapper
    return func(*args, **kwargs)
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/util/_decorators.py", line 331, in wrapper
    return func(*args, **kwargs)
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 950, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 605, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1442, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1735, in _make_engine
    self.handles = get_handle(
  File "/jobfs/116408127.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/common.py", line 856, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: '../../data/pems/pems04_d.csv'
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='pems04_d.csv', dec_in=822, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='DLinear', model_id='pems04_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
Traceback (most recent call last):
  File "run.py", line 158, in <module>
    exp.train(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 84, in train
    train_data, train_loader = self._get_data(flag='train')
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 31, in _get_data
    data_set, data_loader = data_provider(self.args, flag)
  File "/g/data/hn98/du/exlts/hourdayweek/data_provider/data_factory.py", line 74, in data_provider
    data_set = Data(
  File "/g/data/hn98/du/exlts/hourdayweek/data_provider/data_loader.py", line 320, in __init__
    self.__read_data__()
  File "/g/data/hn98/du/exlts/hourdayweek/data_provider/data_loader.py", line 324, in __read_data__
    df_raw = pd.read_csv(os.path.join(self.root_path,
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/util/_decorators.py", line 211, in wrapper
    return func(*args, **kwargs)
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/util/_decorators.py", line 331, in wrapper
    return func(*args, **kwargs)
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 950, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 605, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1442, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1735, in _make_engine
    self.handles = get_handle(
  File "/jobfs/116443174.gadi-pbs/timesnet/lib/python3.8/site-packages/pandas/io/common.py", line 856, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: '../../data/pems/pems04_d.csv'
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='pems04_d.csv', dec_in=822, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='DLinear', model_id='pems04_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5143
val 463
test 1257
	iters: 100, epoch: 1 | loss: 0.6620442
	speed: 0.0616s/iter; left time: 1963.5786s
Epoch: 1 cost time: 9.672862529754639
Epoch: 1, Steps: 160 | Train Loss: 0.7193858 Vali Loss: 0.4951497 Test Loss: 0.6723522
Validation loss decreased (inf --> 0.495150).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.6498196
	speed: 0.1805s/iter; left time: 5729.4835s
Epoch: 2 cost time: 9.395082235336304
Epoch: 2, Steps: 160 | Train Loss: 0.6936894 Vali Loss: 0.4940098 Test Loss: 0.6642227
Validation loss decreased (0.495150 --> 0.494010).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.6435980
	speed: 0.1812s/iter; left time: 5721.5915s
Epoch: 3 cost time: 9.453741550445557
Epoch: 3, Steps: 160 | Train Loss: 0.6919487 Vali Loss: 0.4909276 Test Loss: 0.6629994
Validation loss decreased (0.494010 --> 0.490928).  Saving model ...
	iters: 100, epoch: 4 | loss: 0.6336279
	speed: 0.1796s/iter; left time: 5641.8304s
Epoch: 4 cost time: 9.447235345840454
Epoch: 4, Steps: 160 | Train Loss: 0.6919236 Vali Loss: 0.4924932 Test Loss: 0.6629816
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 5 | loss: 0.7172151
	speed: 0.1805s/iter; left time: 5641.5827s
Epoch: 5 cost time: 9.478471994400024
Epoch: 5, Steps: 160 | Train Loss: 0.6911567 Vali Loss: 0.4922245 Test Loss: 0.6640034
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 6 | loss: 0.6888695
	speed: 0.1802s/iter; left time: 5605.4781s
Epoch: 6 cost time: 9.403456211090088
Epoch: 6, Steps: 160 | Train Loss: 0.6913175 Vali Loss: 0.4918568 Test Loss: 0.6626060
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 7 | loss: 0.7119811
	speed: 0.1801s/iter; left time: 5573.6571s
Epoch: 7 cost time: 9.530043840408325
Epoch: 7, Steps: 160 | Train Loss: 0.6905412 Vali Loss: 0.4915840 Test Loss: 0.6616877
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 8 | loss: 0.6015377
	speed: 0.1808s/iter; left time: 5563.9275s
Epoch: 8 cost time: 9.381534099578857
Epoch: 8, Steps: 160 | Train Loss: 0.6915255 Vali Loss: 0.4907050 Test Loss: 0.6631734
Validation loss decreased (0.490928 --> 0.490705).  Saving model ...
	iters: 100, epoch: 9 | loss: 0.5629792
	speed: 0.1797s/iter; left time: 5501.9855s
Epoch: 9 cost time: 9.451590299606323
Epoch: 9, Steps: 160 | Train Loss: 0.6914867 Vali Loss: 0.4922765 Test Loss: 0.6651756
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 10 | loss: 0.6660625
	speed: 0.1802s/iter; left time: 5488.0856s
Epoch: 10 cost time: 9.432003736495972
Epoch: 10, Steps: 160 | Train Loss: 0.6911484 Vali Loss: 0.4928773 Test Loss: 0.6624566
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 11 | loss: 0.7064360
	speed: 0.1800s/iter; left time: 5453.8497s
Epoch: 11 cost time: 9.467700958251953
Epoch: 11, Steps: 160 | Train Loss: 0.6913893 Vali Loss: 0.4917508 Test Loss: 0.6624134
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 12 | loss: 0.8261676
	speed: 0.1803s/iter; left time: 5434.3801s
Epoch: 12 cost time: 9.415391683578491
Epoch: 12, Steps: 160 | Train Loss: 0.6911524 Vali Loss: 0.4915326 Test Loss: 0.6628680
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 13 | loss: 0.7202048
	speed: 0.1798s/iter; left time: 5391.8889s
Epoch: 13 cost time: 9.443146467208862
Epoch: 13, Steps: 160 | Train Loss: 0.6909151 Vali Loss: 0.4917370 Test Loss: 0.6624244
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1257
Traceback (most recent call last):
  File "run.py", line 161, in <module>
    exp.test(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 286, in test
    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))
  File "/g/data/hn98/du/exlts/hourdayweek/utils/tools.py", line 89, in visual
    plt.savefig(name, bbox_inches='tight')
  File "/jobfs/116444158.gadi-pbs/timesnet/lib/python3.8/site-packages/matplotlib/pyplot.py", line 996, in savefig
    res = fig.savefig(*args, **kwargs)
  File "/jobfs/116444158.gadi-pbs/timesnet/lib/python3.8/site-packages/matplotlib/figure.py", line 3328, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File "/jobfs/116444158.gadi-pbs/timesnet/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 2362, in print_figure
    result = print_method(
  File "/jobfs/116444158.gadi-pbs/timesnet/lib/python3.8/site-packages/matplotlib/backend_bases.py", line 2228, in <lambda>
    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(
  File "/jobfs/116444158.gadi-pbs/timesnet/lib/python3.8/site-packages/matplotlib/backends/backend_pdf.py", line 2823, in print_pdf
    file.close()
  File "/jobfs/116444158.gadi-pbs/timesnet/lib/python3.8/site-packages/matplotlib/backends/backend_pdf.py", line 890, in close
    self.fh.close()
OSError: [Errno 122] Disk quota exceeded
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='pems04_d.csv', dec_in=822, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='DLinear', model_id='pems04_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5143
val 463
test 1257
	iters: 100, epoch: 1 | loss: 0.6620442
	speed: 0.0619s/iter; left time: 1973.9415s
Epoch: 1 cost time: 9.690221548080444
Epoch: 1, Steps: 160 | Train Loss: 0.7193858 Vali Loss: 0.4951497 Test Loss: 0.6723522
Validation loss decreased (inf --> 0.495150).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.6498196
	speed: 0.1809s/iter; left time: 5740.9606s
Epoch: 2 cost time: 9.44556212425232
Epoch: 2, Steps: 160 | Train Loss: 0.6936894 Vali Loss: 0.4940098 Test Loss: 0.6642227
Validation loss decreased (0.495150 --> 0.494010).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.6435980
	speed: 0.1825s/iter; left time: 5762.8871s
Epoch: 3 cost time: 9.575314998626709
Epoch: 3, Steps: 160 | Train Loss: 0.6919487 Vali Loss: 0.4909276 Test Loss: 0.6629994
Validation loss decreased (0.494010 --> 0.490928).  Saving model ...
	iters: 100, epoch: 4 | loss: 0.6336279
	speed: 0.1819s/iter; left time: 5716.6736s
Epoch: 4 cost time: 9.533807277679443
Epoch: 4, Steps: 160 | Train Loss: 0.6919236 Vali Loss: 0.4924932 Test Loss: 0.6629816
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 5 | loss: 0.7172151
	speed: 0.1817s/iter; left time: 5681.0903s
Epoch: 5 cost time: 9.46350884437561
Epoch: 5, Steps: 160 | Train Loss: 0.6911567 Vali Loss: 0.4922245 Test Loss: 0.6640034
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 6 | loss: 0.6888695
	speed: 0.1815s/iter; left time: 5643.4638s
Epoch: 6 cost time: 9.540605545043945
Epoch: 6, Steps: 160 | Train Loss: 0.6913175 Vali Loss: 0.4918568 Test Loss: 0.6626060
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 7 | loss: 0.7119811
	speed: 0.1814s/iter; left time: 5613.2483s
Epoch: 7 cost time: 9.499759435653687
Epoch: 7, Steps: 160 | Train Loss: 0.6905412 Vali Loss: 0.4915840 Test Loss: 0.6616877
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 8 | loss: 0.6015377
	speed: 0.1846s/iter; left time: 5682.0624s
Epoch: 8 cost time: 9.476931810379028
Epoch: 8, Steps: 160 | Train Loss: 0.6915255 Vali Loss: 0.4907050 Test Loss: 0.6631734
Validation loss decreased (0.490928 --> 0.490705).  Saving model ...
	iters: 100, epoch: 9 | loss: 0.5629792
	speed: 0.1814s/iter; left time: 5554.9730s
Epoch: 9 cost time: 9.488626956939697
Epoch: 9, Steps: 160 | Train Loss: 0.6914867 Vali Loss: 0.4922765 Test Loss: 0.6651756
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 10 | loss: 0.6660625
	speed: 0.1847s/iter; left time: 5624.8111s
Epoch: 10 cost time: 9.476229190826416
Epoch: 10, Steps: 160 | Train Loss: 0.6911484 Vali Loss: 0.4928773 Test Loss: 0.6624566
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 11 | loss: 0.7064360
	speed: 0.1816s/iter; left time: 5503.8497s
Epoch: 11 cost time: 9.529380559921265
Epoch: 11, Steps: 160 | Train Loss: 0.6913893 Vali Loss: 0.4917508 Test Loss: 0.6624134
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 12 | loss: 0.8261676
	speed: 0.1820s/iter; left time: 5484.4277s
Epoch: 12 cost time: 9.522105693817139
Epoch: 12, Steps: 160 | Train Loss: 0.6911524 Vali Loss: 0.4915326 Test Loss: 0.6628680
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 13 | loss: 0.7202048
	speed: 0.1815s/iter; left time: 5440.5642s
Epoch: 13 cost time: 9.498508214950562
Epoch: 13, Steps: 160 | Train Loss: 0.6909151 Vali Loss: 0.4917370 Test Loss: 0.6624244
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1257
test shape: (1257, 1, 336, 822) (1257, 1, 336, 822)
test shape: (1257, 336, 822) (1257, 336, 822)
mse:0.6631762981414795, mae:0.5218226313591003
>>>>>>>Overall time: 264 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=822, checkpoints='./checkpoints/', d_conv=4, d_ff=2048, d_layers=1, d_model=512, data='custom', data_path='pems04_d.csv', dec_in=822, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=822, expand=2, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='DLinear', model_id='pems04_d_96_336', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=336, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5143
val 463
test 1257
	iters: 100, epoch: 1 | loss: 0.6620442
	speed: 0.0606s/iter; left time: 1934.5823s
Epoch: 1 cost time: 9.495879888534546
Epoch: 1, Steps: 160 | Train Loss: 0.7193858 Vali Loss: 0.4951497 Test Loss: 0.6723522
Validation loss decreased (inf --> 0.495150).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.6498196
	speed: 0.1802s/iter; left time: 5718.6447s
Epoch: 2 cost time: 9.4869065284729
Epoch: 2, Steps: 160 | Train Loss: 0.6936894 Vali Loss: 0.4940098 Test Loss: 0.6642227
Validation loss decreased (0.495150 --> 0.494010).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.6435980
	speed: 0.1802s/iter; left time: 5691.5974s
Epoch: 3 cost time: 9.389195442199707
Epoch: 3, Steps: 160 | Train Loss: 0.6919487 Vali Loss: 0.4909276 Test Loss: 0.6629994
Validation loss decreased (0.494010 --> 0.490928).  Saving model ...
	iters: 100, epoch: 4 | loss: 0.6336279
	speed: 0.1796s/iter; left time: 5643.1557s
Epoch: 4 cost time: 9.43164348602295
Epoch: 4, Steps: 160 | Train Loss: 0.6919236 Vali Loss: 0.4924932 Test Loss: 0.6629816
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 5 | loss: 0.7172151
	speed: 0.1809s/iter; left time: 5656.2976s
Epoch: 5 cost time: 9.391579151153564
Epoch: 5, Steps: 160 | Train Loss: 0.6911567 Vali Loss: 0.4922245 Test Loss: 0.6640034
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 6 | loss: 0.6888695
	speed: 0.1807s/iter; left time: 5621.2273s
Epoch: 6 cost time: 9.450643301010132
Epoch: 6, Steps: 160 | Train Loss: 0.6913175 Vali Loss: 0.4918568 Test Loss: 0.6626060
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 7 | loss: 0.7119811
	speed: 0.1809s/iter; left time: 5597.1948s
Epoch: 7 cost time: 9.409178256988525
Epoch: 7, Steps: 160 | Train Loss: 0.6905412 Vali Loss: 0.4915840 Test Loss: 0.6616877
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 8 | loss: 0.6015377
	speed: 0.1800s/iter; left time: 5541.9673s
Epoch: 8 cost time: 9.44390344619751
Epoch: 8, Steps: 160 | Train Loss: 0.6915255 Vali Loss: 0.4907050 Test Loss: 0.6631734
Validation loss decreased (0.490928 --> 0.490705).  Saving model ...
	iters: 100, epoch: 9 | loss: 0.5629792
	speed: 0.1803s/iter; left time: 5521.4543s
Epoch: 9 cost time: 9.346565246582031
Epoch: 9, Steps: 160 | Train Loss: 0.6914867 Vali Loss: 0.4922765 Test Loss: 0.6651756
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 10 | loss: 0.6660625
	speed: 0.1810s/iter; left time: 5512.2860s
Epoch: 10 cost time: 9.495408058166504
Epoch: 10, Steps: 160 | Train Loss: 0.6911484 Vali Loss: 0.4928773 Test Loss: 0.6624566
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 11 | loss: 0.7064360
	speed: 0.1816s/iter; left time: 5504.0402s
Epoch: 11 cost time: 9.535111665725708
Epoch: 11, Steps: 160 | Train Loss: 0.6913893 Vali Loss: 0.4917508 Test Loss: 0.6624134
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 12 | loss: 0.8261676
	speed: 0.1813s/iter; left time: 5464.2473s
Epoch: 12 cost time: 9.447423934936523
Epoch: 12, Steps: 160 | Train Loss: 0.6911524 Vali Loss: 0.4915326 Test Loss: 0.6628680
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 13 | loss: 0.7202048
	speed: 0.1791s/iter; left time: 5369.4320s
Epoch: 13 cost time: 9.29493522644043
Epoch: 13, Steps: 160 | Train Loss: 0.6909151 Vali Loss: 0.4917370 Test Loss: 0.6624244
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems04_d_96_336_DLinear_custom_ftM_sl96_ll48_pl336_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1257
test shape: (1257, 1, 336, 822) (1257, 1, 336, 822)
test shape: (1257, 336, 822) (1257, 336, 822)
mse:0.6631762981414795, mae:0.5218226313591003
>>>>>>>Overall time: 249 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
