Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=192, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl192_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5393
val 621
test 1432
	iters: 100, epoch: 1 | loss: 0.6596637
	speed: 0.2271s/iter; left time: 7607.0758s
Epoch: 1 cost time: 37.37547039985657
Epoch: 1, Steps: 168 | Train Loss: 0.7393084 Vali Loss: 0.7729085 Test Loss: 1.8171438
Validation loss decreased (inf --> 0.772909).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.7023544
	speed: 0.6507s/iter; left time: 21689.2911s
Epoch: 2 cost time: 33.310872316360474
Epoch: 2, Steps: 168 | Train Loss: 0.6703987 Vali Loss: 0.7632988 Test Loss: 1.8533924
Validation loss decreased (0.772909 --> 0.763299).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.6771923
	speed: 0.5861s/iter; left time: 19438.3327s
Epoch: 3 cost time: 32.9928195476532
Epoch: 3, Steps: 168 | Train Loss: 0.6295664 Vali Loss: 0.7753146 Test Loss: 1.8523068
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 4 | loss: 0.6519973
	speed: 0.5872s/iter; left time: 19375.9130s
Epoch: 4 cost time: 33.136727809906006
Epoch: 4, Steps: 168 | Train Loss: 0.6361033 Vali Loss: 0.7680287 Test Loss: 1.8267730
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 5 | loss: 0.5622122
	speed: 0.6279s/iter; left time: 20613.2499s
Epoch: 5 cost time: 40.01681900024414
Epoch: 5, Steps: 168 | Train Loss: 0.5785132 Vali Loss: 0.7825297 Test Loss: 1.8531559
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 6 | loss: 0.5010564
	speed: 0.6672s/iter; left time: 21792.5254s
Epoch: 6 cost time: 33.678879261016846
Epoch: 6, Steps: 168 | Train Loss: 0.5382939 Vali Loss: 0.7786250 Test Loss: 1.8556571
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 7 | loss: 0.4843090
	speed: 0.5863s/iter; left time: 19050.3483s
Epoch: 7 cost time: 32.69814085960388
Epoch: 7, Steps: 168 | Train Loss: 0.5025690 Vali Loss: 0.7872977 Test Loss: 1.8574635
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl192_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1432
test shape: (1432, 1, 192, 1543) (1432, 1, 192, 1543)
test shape: (1432, 192, 1543) (1432, 192, 1543)
mse:1.8533954620361328, mae:0.6752250790596008
>>>>>>>Overall time: 482 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=1543, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=1543, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=1543, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=192, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl192_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5393
val 621
test 1432
Traceback (most recent call last):
  File "run.py", line 158, in <module>
    exp.train(setting)
  File "/g/data/hn98/du/exlts/hourdayweek/exp/exp_long_term_forecasting.py", line 146, in train
    outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 147, in forward
    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
  File "/g/data/hn98/du/exlts/hourdayweek/models/Autoformer.py", line 102, in forecast
    enc_out = self.enc_embedding(x_enc, x_mark_enc)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 161, in forward
    x = self.value_embedding(x) + self.temporal_embedding(x_mark)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/g/data/hn98/du/exlts/hourdayweek/layers/Embed.py", line 41, in forward
    x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/jobfs/116745186.gadi-pbs/timesnet/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 255, in forward
    return F.conv1d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
RuntimeError: Given groups=1, weight of size [128, 1543, 3], expected input[32, 867, 98] to have 1543 channels, but got 867 channels instead
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=32, c_out=867, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=128, data='custom', data_path='pems12_d.csv', dec_in=867, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=2, embed='timeF', enc_in=867, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.0005, loss='MSE', lradj='type3', mask_rate=0.25, model='Autoformer', model_id='pems12_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=5, pred_len=192, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=200, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl192_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 5393
val 621
test 1432
	iters: 100, epoch: 1 | loss: 0.7450905
	speed: 0.1676s/iter; left time: 5613.8418s
Epoch: 1 cost time: 27.338314294815063
Epoch: 1, Steps: 168 | Train Loss: 0.7205937 Vali Loss: 0.7538909 Test Loss: 1.5781195
Validation loss decreased (inf --> 0.753891).  Saving model ...
	iters: 100, epoch: 2 | loss: 0.5873361
	speed: 0.4842s/iter; left time: 16140.4449s
Epoch: 2 cost time: 26.73542308807373
Epoch: 2, Steps: 168 | Train Loss: 0.6209481 Vali Loss: 0.7405320 Test Loss: 1.5537192
Validation loss decreased (0.753891 --> 0.740532).  Saving model ...
	iters: 100, epoch: 3 | loss: 0.5715091
	speed: 0.4869s/iter; left time: 16146.8709s
Epoch: 3 cost time: 27.05285954475403
Epoch: 3, Steps: 168 | Train Loss: 0.5665561 Vali Loss: 0.7364343 Test Loss: 1.5476456
Validation loss decreased (0.740532 --> 0.736434).  Saving model ...
	iters: 100, epoch: 4 | loss: 0.4707794
	speed: 0.4849s/iter; left time: 16001.4575s
Epoch: 4 cost time: 26.62420630455017
Epoch: 4, Steps: 168 | Train Loss: 0.5031595 Vali Loss: 0.7381734 Test Loss: 1.5800229
EarlyStopping counter: 1 out of 5
	iters: 100, epoch: 5 | loss: 0.5969450
	speed: 0.4838s/iter; left time: 15881.5823s
Epoch: 5 cost time: 26.59802746772766
Epoch: 5, Steps: 168 | Train Loss: 0.5376246 Vali Loss: 0.7376621 Test Loss: 1.6913053
EarlyStopping counter: 2 out of 5
	iters: 100, epoch: 6 | loss: 0.5970957
	speed: 0.4821s/iter; left time: 15745.7349s
Epoch: 6 cost time: 26.625786781311035
Epoch: 6, Steps: 168 | Train Loss: 0.5142614 Vali Loss: 0.7597111 Test Loss: 1.6125747
EarlyStopping counter: 3 out of 5
	iters: 100, epoch: 7 | loss: 0.3763013
	speed: 0.4830s/iter; left time: 15693.6492s
Epoch: 7 cost time: 26.76189613342285
Epoch: 7, Steps: 168 | Train Loss: 0.4061952 Vali Loss: 0.7694668 Test Loss: 1.6351724
EarlyStopping counter: 4 out of 5
	iters: 100, epoch: 8 | loss: 0.3575030
	speed: 0.4854s/iter; left time: 15689.0538s
Epoch: 8 cost time: 26.891961097717285
Epoch: 8, Steps: 168 | Train Loss: 0.3712299 Vali Loss: 0.7714746 Test Loss: 1.6768339
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : long_term_forecast_pems12_d_96_96_Autoformer_custom_ftM_sl96_ll48_pl192_dm128_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 1432
test shape: (1432, 1, 192, 867) (1432, 1, 192, 867)
test shape: (1432, 192, 867) (1432, 192, 867)
mse:1.5476478338241577, mae:0.6855524778366089
>>>>>>>Overall time: 418 seconds<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
