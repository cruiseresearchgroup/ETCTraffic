Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=16, c_out=107, checkpoints='./checkpoints/', d_ff=512, d_layers=1, d_model=512, data='custom', data_path='pems10_d.csv', dec_in=107, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=4, embed='timeF', enc_in=107, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.001, loss='MSE', lradj='type1', mask_rate=0.25, model='iTransformer', model_id='pems10_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=10, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems10_d_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl1_df512_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4087
val 517
test 1127
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=16, c_out=107, checkpoints='./checkpoints/', d_ff=512, d_layers=1, d_model=512, data='custom', data_path='pems10_d.csv', dec_in=107, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=4, embed='timeF', enc_in=107, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.001, loss='MSE', lradj='type1', mask_rate=0.25, model='iTransformer', model_id='pems10_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=10, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems10_d_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl1_df512_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4087
val 517
test 1127
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=16, c_out=107, checkpoints='./checkpoints/', d_ff=512, d_layers=1, d_model=512, data='custom', data_path='pems10_d.csv', dec_in=107, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=4, embed='timeF', enc_in=107, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.001, loss='MSE', lradj='type1', mask_rate=0.25, model='iTransformer', model_id='pems10_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=10, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems10_d_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl1_df512_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4087
val 517
test 1127
	iters: 100, epoch: 1 | loss: 0.5639021
	speed: 0.0359s/iter; left time: 87.8862s
	iters: 200, epoch: 1 | loss: 0.5064141
	speed: 0.0302s/iter; left time: 71.0802s
Epoch: 1 cost time: 8.371923208236694
Epoch: 1, Steps: 255 | Train Loss: 0.5351124 Vali Loss: 1.2373136 Test Loss: 0.9959159
Validation loss decreased (inf --> 1.237314).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.4290530
	speed: 0.1172s/iter; left time: 257.4236s
	iters: 200, epoch: 2 | loss: 0.4296623
	speed: 0.0350s/iter; left time: 73.3785s
Epoch: 2 cost time: 9.257592916488647
Epoch: 2, Steps: 255 | Train Loss: 0.5118344 Vali Loss: 1.2397437 Test Loss: 1.0110663
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.6028732
	speed: 0.1162s/iter; left time: 225.5505s
	iters: 200, epoch: 3 | loss: 0.7178512
	speed: 0.0305s/iter; left time: 56.1252s
Epoch: 3 cost time: 8.317041397094727
Epoch: 3, Steps: 255 | Train Loss: 0.4787540 Vali Loss: 1.2173734 Test Loss: 0.9667302
Validation loss decreased (1.237314 --> 1.217373).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.3568081
	speed: 0.1088s/iter; left time: 183.3603s
	iters: 200, epoch: 4 | loss: 0.5270972
	speed: 0.0356s/iter; left time: 56.4768s
Epoch: 4 cost time: 8.60004210472107
Args in experiment:
Namespace(activation='gelu', anomaly_ratio=0.25, batch_size=16, c_out=107, checkpoints='./checkpoints/', d_ff=512, d_layers=1, d_model=512, data='custom', data_path='pems10_d.csv', dec_in=107, des='Exp', devices='0,1,2,3', distil=True, dropout=0, e_layers=4, embed='timeF', enc_in=107, factor=3, features='M', freq='h', gap_day=365, gpu=0, inverse=False, is_training=1, itr=1, label_len=48, learning_rate=0.001, loss='MSE', lradj='type1', mask_rate=0.25, model='iTransformer', model_id='pems10_d_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, patience=3, pred_len=96, root_path='../../data/pems/', samle_rate=1.0, sample_seed=7, seasonal_patterns='Monthly', seq_len=96, target='OT', task_name='long_term_forecast', top_k=5, train_epochs=10, train_seed=2024, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_pems10_d_96_96_iTransformer_custom_ftM_sl96_ll48_pl96_dm512_nh8_el4_dl1_df512_fc3_ebtimeF_dtTrue_srate1.0_sseed7_trainseed2024_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 4087
val 517
test 1127
	iters: 100, epoch: 1 | loss: 0.5639021
	speed: 0.0344s/iter; left time: 84.4123s
	iters: 200, epoch: 1 | loss: 0.5064141
	speed: 0.0299s/iter; left time: 70.2705s
Epoch: 1 cost time: 8.182984590530396
Epoch: 1, Steps: 255 | Train Loss: 0.5351124 Vali Loss: 1.2373136 Test Loss: 0.9959159
Validation loss decreased (inf --> 1.237314).  Saving model ...
Updating learning rate to 0.001
	iters: 100, epoch: 2 | loss: 0.4290530
	speed: 0.1068s/iter; left time: 234.5208s
	iters: 200, epoch: 2 | loss: 0.4296623
	speed: 0.0298s/iter; left time: 62.3808s
Epoch: 2 cost time: 7.924317836761475
Epoch: 2, Steps: 255 | Train Loss: 0.5118344 Vali Loss: 1.2397437 Test Loss: 1.0110663
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0005
	iters: 100, epoch: 3 | loss: 0.6028732
	speed: 0.1042s/iter; left time: 202.1824s
	iters: 200, epoch: 3 | loss: 0.7178512
	speed: 0.0304s/iter; left time: 56.0245s
Epoch: 3 cost time: 8.0404212474823
Epoch: 3, Steps: 255 | Train Loss: 0.4787540 Vali Loss: 1.2173734 Test Loss: 0.9667302
Validation loss decreased (1.237314 --> 1.217373).  Saving model ...
Updating learning rate to 0.00025
	iters: 100, epoch: 4 | loss: 0.3568081
	speed: 0.1050s/iter; left time: 177.0870s
	iters: 200, epoch: 4 | loss: 0.5270972
	speed: 0.0300s/iter; left time: 47.5831s
Epoch: 4 cost time: 7.9401843547821045
Epoch: 4, Steps: 255 | Train Loss: 0.4522709 Vali Loss: 1.1419675 Test Loss: 0.9255596
Validation loss decreased (1.217373 --> 1.141968).  Saving model ...
Updating learning rate to 0.000125
